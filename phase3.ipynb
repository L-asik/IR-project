{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import L\n",
    "from sklearn import metrics\n",
    "import rank_metric as metrics\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "import tarfile\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from bertviz import model_view, head_view\n",
    "import torch\n",
    "from transformers import *\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/anteklasik/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/anteklasik/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/anteklasik/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/anteklasik/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/anteklasik/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/anteklasik/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/anteklasik/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_path = 'bert-base-uncased'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path,  output_hidden_states=True, output_attentions=True)  \n",
    "model = AutoModel.from_pretrained(model_path, config=config).to(device)\n",
    "# model = AutoModel.from_pretrained(model_path, config=config).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trials to use: 3626\n"
     ]
    }
   ],
   "source": [
    "#parsing real patient cases\n",
    "import pickle\n",
    "import os\n",
    "trials = \"data/qrels-clinical_trials.txt\"\n",
    "trials = pd.read_csv(trials, sep='\\t', names=[\"query_id\", \"dummy\", \"docid\", \"rel\"])\n",
    "trials = trials.drop(\"dummy\", axis = 1)\n",
    "trials_to_use = set(trials['docid'])\n",
    "if not os.path.exists(\"data/doc_brief_title.bin\") or not os.path.exists(\"data/doc_ids.bin\") or not os.path.exists(\"data/docs_detailed_description.bin\") or not os.path.exists(\"data/docs_brief_summary.bin\") or not os.path.exists(\"data/docs_criteria.bin\"):\n",
    "    #parsing clinical trials\n",
    "    \n",
    "    tar = tarfile.open(\"data/clinicaltrials.gov-16_dec_2015.tgz\", \"r:gz\")\n",
    "    clinical_trials = {}\n",
    "    docs_brief_title = {}\n",
    "    docs_detailed_description={}\n",
    "    docs_brief_summary={}\n",
    "    docs_criteria={}\n",
    "    ids_brief_title = []\n",
    "    ids_detailed_description=[]\n",
    "    ids_brief_summary=[]\n",
    "    ids_criteria=[]\n",
    "\n",
    "    i = 0\n",
    "    for element in tar:\n",
    "        if element.size > 500:\n",
    "            txt = tar.extractfile(element).read().decode(\"utf-8\", \"strict\")\n",
    "            root = ET.fromstring(txt)\n",
    "            for doc_id in root.iter('nct_id'):\n",
    "                temp_id = doc_id.text\n",
    "            if temp_id not in trials_to_use:\n",
    "                continue\n",
    "            i+=1\n",
    "            for brief_title in root.iter('brief_title'):\n",
    "                id_title=doc_id.text\n",
    "                docs_brief_title[temp_id]=brief_title.text\n",
    "                ids_brief_title.append(temp_id)\n",
    "\n",
    "            \n",
    "            \n",
    "            for brief_summary in root.iter('brief_summary'):\n",
    "                for child in brief_summary:\n",
    "                    docs_brief_summary[temp_id]=child.text.strip()\n",
    "                    # ids_brief_summary.append(temp_id)\n",
    "                    id_sum=doc_id.text\n",
    "            if temp_id!=id_sum:\n",
    "                \n",
    "                docs_brief_summary[temp_id]=docs_brief_title[temp_id]\n",
    "\n",
    "\n",
    "            for detailed_description in root.iter('detailed_description'):\n",
    "                for child in detailed_description:\n",
    "                    docs_detailed_description[temp_id]=child.text.strip()\n",
    "                    # ids_detailed_description.append(temp_id)\n",
    "                    id_detailed=doc_id.text\n",
    "\n",
    "            if temp_id!=id_detailed:\n",
    "                \n",
    "                docs_detailed_description[temp_id]=docs_brief_title[temp_id]+docs_brief_summary[temp_id]\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            for criteria in root.iter('criteria'):\n",
    "                for child in criteria:\n",
    "                    id_crit=doc_id.text\n",
    "                    doc=child.text.strip().split(\"Exclusion\")\n",
    "                    doc=doc[0].split()\n",
    "                    temp=[]\n",
    "                    for x in doc:\n",
    "                        if x==\"-\" or x==\"Inclusion\" or x==\"Criteria:\":\n",
    "                            continue\n",
    "                        temp.append(x)\n",
    "                    doc=' '.join(temp)\n",
    "                    if len(doc)==0:\n",
    "                        docs_criteria[temp_id]=docs_detailed_description[temp_id]\n",
    "                    else:\n",
    "                        docs_criteria[temp_id]=doc\n",
    "\n",
    "                    # ids_criteria.append(temp_id)\n",
    "            if temp_id!=id_crit:\n",
    "                \n",
    "                docs_criteria[temp_id]=docs_detailed_description[temp_id]\n",
    "\n",
    "                    \n",
    "            \n",
    "    print(f\"Number of trials to use: {i}\")\n",
    "\n",
    "    pickle.dump(trials,open(\"data/trials.bin\",\"wb\"))\n",
    "    pickle.dump(docs_brief_title, open( \"data/docs_brief_title.bin\", \"wb\" ) )\n",
    "    pickle.dump(ids_brief_title, open( \"data/doc_ids.bin\", \"wb\" ) )\n",
    "    pickle.dump(docs_brief_summary,open( \"data/docs_brief_summary.bin\", \"wb\" ))\n",
    "    pickle.dump(docs_detailed_description,open( \"data/docs_detailed_description.bin\", \"wb\" ))\n",
    "    pickle.dump(docs_criteria,open( \"data/docs_criteria.bin\", \"wb\" ))\n",
    "    pickle.dump(ids_brief_summary, open( \"data/doc_ids_summary.bin\", \"wb\" ) )\n",
    "    pickle.dump(ids_detailed_description, open( \"data/doc_ids_description.bin\", \"wb\" ) )\n",
    "    pickle.dump(ids_criteria, open( \"data/doc_ids_criteria.bin\", \"wb\" ) )\n",
    "else:\n",
    "    trials=pickle.load(open(\"data/trials.bin\",\"rb\"))\n",
    "    docs_brief_title = pickle.load( open( \"data/docs_brief_title.bin\", \"rb\" ) )\n",
    "    ids_brief_title = pickle.load( open( \"data/doc_ids.bin\", \"rb\" ) )\n",
    "    docs_detailed_description=pickle.load( open( \"data/docs_detailed_description.bin\", \"rb\" ) )\n",
    "    docs_brief_summary=pickle.load( open( \"data/docs_brief_summary.bin\", \"rb\" ) )\n",
    "    docs_criteria=pickle.load( open( \"data/docs_criteria.bin\", \"rb\" ) )\n",
    "    # ids_detailed_description=pickle.load( open( \"data/doc_ids_description.bin\", \"rb\" ) )\n",
    "    # ids_brief_summary=pickle.load( open( \"data/doc_ids_summary.bin\", \"rb\" ) )\n",
    "    # ids_criteria=pickle.load( open( \"data/doc_ids_criteria.bin\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3626 <class 'list'>\n",
      "3626 <class 'list'>\n",
      "3626 <class 'list'>\n",
      "3626 <class 'list'>\n",
      "3870 <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dict_detailed=docs_detailed_description\n",
    "docs_brief_summary=list(docs_brief_summary.values())\n",
    "docs_brief_title=list(docs_brief_title.values())\n",
    "docs_detailed_description=list(docs_detailed_description.values())\n",
    "docs_criteria=list(docs_criteria.values())\n",
    "print(len(docs_brief_summary),type(docs_brief_summary))\n",
    "print(len(docs_brief_title),type(docs_brief_title))\n",
    "print(len(docs_detailed_description),type(docs_detailed_description))\n",
    "print(len(docs_criteria),type(docs_criteria))\n",
    "print(len(trials),type(trials))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries = \"data/topics-2014_2015-summary.topics\"\n",
    "with open(Queries, 'r') as queries_reader:\n",
    "    txt = queries_reader.read()\n",
    "root = ET.fromstring(txt)\n",
    "cases = {}\n",
    "for q in root.iter('TOP'):\n",
    "    q_number = q.find(\"NUM\").text\n",
    "    q_title = q.find('TITLE').text\n",
    "    cases[q_number] = q_title"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for functions used by Language Model with Jelineck-Mercer smooting and Vector Space Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP:\n",
    "    def __init__(self, eval_table: pd.DataFrame, cases: dict, ngram_range, docs_brief_title, ids):\n",
    "        # self.docs_brief_title = documents\n",
    "        self.eval_table = eval_table\n",
    "        self.docs_brief_title = docs_brief_title\n",
    "        self.ids = ids\n",
    "        self.cases = cases\n",
    "        self.vsm = TfidfVectorizer(ngram_range=ngram_range, analyzer='word', stop_words = None)\n",
    "        self.X = self.vsm.fit_transform(docs_brief_title)\n",
    "        self.vectorizer = CountVectorizer(ngram_range=(1,1), analyzer='word', stop_words = None)\n",
    "        self.Y=self.vectorizer.fit_transform(docs_brief_title)\n",
    "\n",
    "    def test_vsm(self, query):\n",
    "        tfidf = self.vsm.transform([query])\n",
    "        score = 1 - pairwise_distances(self.X, tfidf, metric='cosine')\n",
    "        for x in range(len(score)):\n",
    "            score[x]=float(score[x])\n",
    "        results = pd.DataFrame(list(zip(self.ids, score)), columns=['_id', 'score'])\n",
    "        ordered_results = results.sort_values(by=['score'], ascending=False).reset_index(drop=True)\n",
    "        return ordered_results,results\n",
    "\n",
    "    def eval(self, result: pd.DataFrame, id):\n",
    "        def_sim = self.eval_table[self.eval_table[\"query_id\"] == int(id)]\n",
    "        def_sim = def_sim[def_sim[\"rel\"] != 0]\n",
    "        rel_ids = def_sim[\"docid\"]\n",
    "        num_of_rel = def_sim[\"rel\"].count()\n",
    "        num_of_results = result[\"_id\"].count()\n",
    "\n",
    "        if num_of_rel == 0:\n",
    "            return [0, 0, 0, 0, 0]\n",
    "\n",
    "        #precision@10\n",
    "        top10 = result['_id'][:10]\n",
    "        p10 = np.intersect1d(top10, rel_ids).size / 10\n",
    "\n",
    "        #recall\n",
    "        recall = np.size(np.intersect1d(result[\"_id\"][:100], rel_ids)) / num_of_rel\n",
    "        rel_res_vector = np.zeros((num_of_results,))\n",
    "        for index, row in def_sim.iterrows():\n",
    "            rel_res_vector = rel_res_vector + ((result['_id'] == row.docid)*row.rel).to_numpy()\n",
    "        #ndcg5\n",
    "        ndcg5 = metrics.ndcg_at_k(r = rel_res_vector, k = 5, method = 1)\n",
    "        ap = metrics.average_precision(rel_res_vector, num_of_rel)\n",
    "        mrr = metrics.mean_reciprocal_rank(rel_res_vector)\n",
    "\n",
    "        return [p10, 1 - recall, ndcg5, ap, mrr]\n",
    "\n",
    "    def evalPR(self, scores, query_id):\n",
    "\n",
    "        aux = self.eval_table.loc[self.eval_table['query_id'] == int(query_id)]\n",
    "        idx_rel_docs_brief_title = aux.loc[aux['rel'] != (0)]\n",
    "\n",
    "        [dummyA, rank_rel, dummyB] = np.intersect1d(scores['_id'], idx_rel_docs_brief_title['docid'], return_indices=True)\n",
    "        rank_rel = np.sort(rank_rel) + 1\n",
    "        total_relv_ret = rank_rel.shape[0]\n",
    "        if total_relv_ret == 0:\n",
    "            return [np.zeros(11, ), [], total_relv_ret]\n",
    "\n",
    "        recall = np.arange(1, total_relv_ret + 1) / idx_rel_docs_brief_title.shape[0]\n",
    "        precision = np.arange(1, total_relv_ret + 1) / rank_rel\n",
    "\n",
    "        precision_interpolated = np.maximum.accumulate(precision)\n",
    "        recall_11point = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "        precision_11point = np.interp(recall_11point, recall, precision)\n",
    "\n",
    "        if False:\n",
    "            print(total_relv_ret)\n",
    "            print(rank_rel)\n",
    "            print(recall)\n",
    "            print(precision)\n",
    "            plt.plot(recall, precision, color='b', alpha=1)  # Raw precision-recall\n",
    "            plt.plot(recall, precision_interpolated, color='r', alpha=1)  # Interpolated precision-recall\n",
    "            plt.plot(recall_11point, precision_11point, color='g', alpha=1)  # 11-point interpolated precision-recall\n",
    "\n",
    "        return [precision_11point, recall_11point, total_relv_ret]\n",
    "\n",
    "\n",
    "    def words_and_indexes_association(self,query):\n",
    "        q_words=[]\n",
    "        word_idx=[]\n",
    "        all_words=self.vectorizer.get_feature_names()\n",
    "        for x in range(len(query)):\n",
    "            for y in range(len(all_words)):\n",
    "                if query[x] == all_words[y]:\n",
    "                    q_words.append(query[x])\n",
    "                    word_idx.append(y)\n",
    "                    break\n",
    "        return word_idx\n",
    "\n",
    "    def JMS(self):\n",
    "        x_array=self.Y.toarray()\n",
    "        x_array=np.array(x_array)\n",
    "        #sum of rows\n",
    "        p1=np.sum(x_array,axis=0)\n",
    "        #sum of cols\n",
    "        p2=np.sum(x_array,axis=1)\n",
    "        #number of words in whole corpus\n",
    "        p3=np.sum(x_array)\n",
    "        #probability of term in corpus\n",
    "        matrix_corpus=np.divide(p1,p3)\n",
    "        #probability of term in document\n",
    "        p2=np.reshape(p2,(p2.size,1))\n",
    "        matrix_documents=np.divide(x_array,p2)  \n",
    "        return matrix_corpus,matrix_documents\n",
    "\n",
    "    def query_formating(self,query):\n",
    "        query=query[:-11]\n",
    "        for y in query:\n",
    "                if y in \".,\":\n",
    "                    query=query.replace(y,'')\n",
    "                \n",
    "        return query \n",
    "\n",
    "    def prob_with_JMS(self,index_word,index_doc,lamb,matrix_corp,matrix_doc):\n",
    "        prob=0\n",
    "        for x in index_word:\n",
    "            #print(x,index_doc)\n",
    "        \n",
    "            p_md=matrix_doc[index_doc][x]\n",
    "            p_mc=matrix_corp[x]\n",
    "            prob+=math.log(lamb*p_md+(1-lamb)*p_mc)\n",
    "        return prob\n",
    "\n",
    "    def JSM_test(self,query,lamb,matrix_corpus,matrix_documents):\n",
    "        scores_now=[]\n",
    "        q=query.split()\n",
    "        word_idx=self.words_and_indexes_association(q)\n",
    "        #print(len(self.ids))\n",
    "        for x in range(len(self.ids)):\n",
    "            #print(word_idx,x,lamb)\n",
    "            prob=self.prob_with_JMS(word_idx,x,lamb,matrix_corpus,matrix_documents)\n",
    "            scores_now.append(prob)\n",
    "        results = pd.DataFrame(list(zip(self.ids, scores_now)), columns=['_id', 'score'])\n",
    "        ordered_results = results.sort_values(by=['score'], ascending=False).reset_index(drop=True)\n",
    "        return ordered_results,results\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cases_training=pickle.load(open(\"data/cases_training.bin\",\"rb\"))\n",
    "cases_test=pickle.load(open(\"data/cases_test.bin\",\"rb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def extract_cls(pairs,query_pairs, embeddings, batch_size=32):\n",
    "    \n",
    "\n",
    "    # Iterate over all documents, in batches of size <batch_size>\n",
    "    for batch_idx in range(0, len(query_pairs), batch_size):\n",
    "        \n",
    "\n",
    "        # Get the current batch of samples\n",
    "        batch_data = query_pairs[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "        inputs = tokenizer.batch_encode_plus(batch_data, \n",
    "                                       return_tensors='pt',  # pytorch tensors\n",
    "                                       add_special_tokens=True,  # Add CLS and SEP tokens\n",
    "                                       max_length = 512, # Max sequence length\n",
    "                                       truncation = True, # Truncate if sequences exceed the Max Sequence length\n",
    "                                       padding = True) # Add padding to forward sequences with different lengths\n",
    "        \n",
    "        # Forward the batch of (query, doc) sequences\n",
    "        with torch.no_grad():\n",
    "            inputs.to(device)\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Get the CLS embeddings for each pair query, document\n",
    "        batch_cls = outputs['hidden_states'][-1][:,0,:]\n",
    "        \n",
    "        # L2-Normalize CLS embeddings. Embeddings norm will be 1.\n",
    "        batch_cls = torch.nn.functional.normalize(batch_cls, p=2, dim=1)\n",
    "        \n",
    "        # Store the extracted CLS embeddings from the batch on the memory-mapped ndarray\n",
    "        embeddings[batch_idx:batch_idx + batch_size] = batch_cls.cpu()\n",
    "        \n",
    "        \n",
    "    embeddings=pd.DataFrame(embeddings)\n",
    "    embeddings[\"pairs\"]=pairs\n",
    "    \n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pairs(cases_training):\n",
    "    dictio={}\n",
    "    \n",
    "    h=0\n",
    "    #add column pair in which we have tuple (document id, query id)\n",
    "    \n",
    "    trials=pickle.load(open(\"data/trials.bin\",\"rb\"))\n",
    "    for x in trials.iterrows():\n",
    "        dictio[str(x[1][0])]=[]\n",
    "    for x in trials.iterrows():\n",
    "      \n",
    "        # for each row i qrels file dict[query].append all of the documents that are labelled\n",
    "        \n",
    "        dictio[str(x[1][0])].append(str(x[1][1]))\n",
    "    k=[]  \n",
    "    \n",
    "    for x in cases_training.items():\n",
    "        scores=pd.DataFrame()\n",
    "        \n",
    "        #if query is qrels file\n",
    "        if x[0] in dictio:\n",
    "            temp=dictio[x[0]]\n",
    "            \n",
    "            \n",
    "            \n",
    "           \n",
    "            l=[]\n",
    "            for z in ids_brief_title:\n",
    "                 l.append((z,str(x[0])))\n",
    "            \n",
    "            scores[\"pairs\"]=l\n",
    "            #add column with field name+vsm or jms ending with corresponding scores\n",
    "            #for each row in these results find these that are labelled in qrels file and put them in relscores dataframe\n",
    "            for z in scores.iterrows():\n",
    "                \n",
    "                \n",
    "                if z[1][0][0] in temp:\n",
    "                    # print(z[1][0][0])\n",
    "                    # h+=1\n",
    "                    # print(h)\n",
    "\n",
    "                    k.append(z[1][0])\n",
    "    \n",
    "    \n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs(cases_training):\n",
    "    k=query_pairs(cases_training)\n",
    "    l=[]\n",
    "    # print(k)\n",
    "    myNLP=NLP(trials, cases_training, (1,1), docs_detailed_description, ids_brief_title)\n",
    "    for x in k:\n",
    "        l.append((myNLP.query_formating(cases_training[x[1]]),dict_detailed[x[0]]))\n",
    "    return l,k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pairs(cases):\n",
    "    k=[]\n",
    "    l=[]\n",
    "    for x in cases.items():\n",
    "\n",
    "        for y in ids_brief_title:\n",
    "            k.append((y,x[0]))\n",
    "    myNLP=NLP(trials, cases, (1,1), docs_detailed_description, ids_brief_title)\n",
    "    for x in k:\n",
    "        l.append((myNLP.query_formating(cases[x[1]]),dict_detailed[x[0]]))\n",
    "    return l,k\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing all of the scores for labelled pair (query, document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Y definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on matrix y (final results dataframe) we make a matrix y in which we have binary labels for each \n",
    "# (query, document) pair from the previous matrix\n",
    "def matrix_y_def(final_scores):\n",
    "    print(type(final_scores))\n",
    "    list_of_relevances=[]\n",
    "    binary_relevance=[]\n",
    "    # final_scores=pickle.load(open(\"data/final_relevant_scores.bin\",\"rb\"))\n",
    "    trials=pickle.load(open(\"data/trials.bin\",\"rb\"))\n",
    "\n",
    "    for f,y in final_scores.iterrows():\n",
    "            for x in trials.iterrows():\n",
    "                #here we use the pairs column to find the corresponding label in qrels file\n",
    "                # if (x[1][1],str(x[1][0]))==y[1][0]:\n",
    "                # print(x[1][1],str(x[1][0]))\n",
    "                # print(y[\"pairs\"])\n",
    "                if (x[1][1],str(x[1][0]))==y[\"pairs\"]:\n",
    "                    \n",
    "                    z=x[1][2]\n",
    "                    #if the label is equal to 2 we change it to 1 \n",
    "                    list_of_relevances.append(z) \n",
    "                    if z>0:\n",
    "                        z=1\n",
    "                        \n",
    "                    \n",
    "                    binary_relevance.append(z)          \n",
    "    # print(len(list_of_relevances))\n",
    "    pickle.dump(list_of_relevances,open(\"data/matrix_y.bin\",\"wb\"))\n",
    "    return list_of_relevances,binary_relevance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix X normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression \n",
    "## using X and Y matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_regr_train(matrix_x,c,typ,matrix_y,rel):\n",
    "    #using both matrices with logistic regression\n",
    "    # matrix_y=pickle.load(open(\"data/matrix_y.bin\",\"rb\"))\n",
    "   \n",
    "    if typ==\"a\":\n",
    "        clf = LogisticRegression(C=c,class_weight=None).fit(matrix_x, matrix_y, sample_weight=None)\n",
    "    if typ==\"b\":\n",
    "        clf = LogisticRegression(C=c,class_weight=\"balanced\").fit(matrix_x, matrix_y, sample_weight=None)\n",
    "    if typ==\"c\":\n",
    "        clf = LogisticRegression(C=c,class_weight=None).fit(matrix_x, matrix_y, sample_weight=rel)\n",
    "    #coeficients correspond to weight we should give each column in matrix X\n",
    "    # (how important are the scores obtaned from each document field)\n",
    "    return clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the scores using logistic regression weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores_embed(coef,cases,c):\n",
    "    records=[]\n",
    "    \n",
    "    prec = np.zeros(11,)\n",
    "    test = np.zeros(5,)\n",
    "    myNLP=NLP(trials, cases, (1,1), docs_detailed_description, ids_brief_title)\n",
    "    for x in cases.items():\n",
    "        scores=pd.DataFrame()\n",
    "        result=pd.DataFrame()\n",
    "        statistics_embed={}\n",
    "        doc_list=[]\n",
    "        zeros=np.zeros(len(ids_brief_title))\n",
    "        z={}\n",
    "        z[x[0]]=x[1]\n",
    "\n",
    "        l,k=all_pairs(z)\n",
    "        for y in k:\n",
    "            doc_list.append(y[0])\n",
    "\n",
    "        matrix_x=np.zeros((len(l), 768))\n",
    "\n",
    "        matrix_x=extract_cls(k,l,matrix_x,batch_size=32)\n",
    "        # print(matrix_x[\"pairs\"])\n",
    "        del matrix_x[\"pairs\"]\n",
    "        result=matrix_x*coef[0]\n",
    "\n",
    "        scores[\"score\"]=result.sum(axis=1)\n",
    "        \n",
    "        # scores[\"pairs\"]=j\n",
    "        # print(scores)\n",
    "        \n",
    "        scores[\"_id\"]=doc_list\n",
    "        # print(scores)\n",
    "        scores=scores.sort_values(by=[\"score\"],ascending=False).reset_index(drop=True)\n",
    "        # print(scores)\n",
    "        p10, recall, ndcg5, ap, mrr = myNLP.eval(scores, x[0])\n",
    "        test +=np.array([p10, ndcg5, mrr, ap, recall])\n",
    "\n",
    "        prec_11point, recal_11point, tot_relv_ret = myNLP.evalPR(scores, x[0])\n",
    "        prec +=prec_11point\n",
    "    prec = prec / (len(cases.keys()))\n",
    "    test = test / (len(cases.keys()))\n",
    "    for p, r in zip(prec, recal_11point):\n",
    "        records.append((c, p, r))\n",
    "    statistics_embed[\"C=\"+str(c)]=test\n",
    "    stats=pd.DataFrame.from_dict(statistics_embed, orient=\"index\", columns=[\"p10\", \"ndcg5\", \"mrr\", \"ap\", \"recall\"])\n",
    "    return stats,records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zaczął tworzyć x\n",
      "zaczął tworzyć y\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Stats\n",
      "3626\n"
     ]
    }
   ],
   "source": [
    "ids_training_cases=list(cases_training.keys())\n",
    "final_stats=pd.DataFrame()\n",
    "for c in [0.1,0.3,0.5,0.7,0.9]:\n",
    "    kf = KFold(3, shuffle=True, random_state=23)\n",
    "    stats=np.zeros((1,5))\n",
    "    for id_train,id_valid in kf.split(ids_training_cases,ids_training_cases):\n",
    "        \n",
    "        \n",
    "        training_set={}\n",
    "        temp=[]\n",
    "        validation_set={}\n",
    "        for id in id_train:\n",
    "            temp=list(cases_training.keys())\n",
    "            training_set[temp[id]] = cases_training[temp[id]]\n",
    "        for id in id_valid:\n",
    "            id=int(id)\n",
    "            validation_set[temp[id]] = cases_training[temp[id]]\n",
    "        # print(\"set\")\n",
    "        # print(len(training_set))\n",
    "        # print(\"pary\")\n",
    "        l,k=pairs(training_set)\n",
    "        # print(len(l),len(k))\n",
    "\n",
    "\n",
    "        matrix_x=np.zeros((len(l), 768))\n",
    "        # print(\"zaczął tworzyć x\")\n",
    "        matrix_x=extract_cls(k,l,matrix_x,batch_size=32)\n",
    "\n",
    "        # print(\"zaczął tworzyć y\")\n",
    "        matrix_y,binary=matrix_y_def(matrix_x)\n",
    "        # matrix_x=matrix_x_norm()\n",
    "        # print(len(matrix_x))\n",
    "        # print(len(matrix_y))\n",
    "        # print(matrix_x)\n",
    "        \n",
    "        del matrix_x[\"pairs\"]\n",
    "        coef=log_regr_train(matrix_x,c,\"a\",binary,matrix_y)\n",
    "        pickle.dump(matrix_x,open(\"data/matirex.bin\",\"wb\"))\n",
    "        pickle.dump(coef,open(\"data/coef.bin\",\"wb\"))\n",
    "        pickle.dump(c,open(\"data/c.bin\",\"wb\"))\n",
    "        \n",
    "        print(\"Stats\")\n",
    "        # temp,records=calc_scores_embed(coef,validation_set,[0.15,0.25,0.4,0.15],c)\n",
    "        temp,records=calc_scores_embed(coef,validation_set,c)\n",
    "        stats+=temp\n",
    "        # print(\"_____________\")\n",
    "    print(\"Stats for c=\",c)\n",
    "    stats=stats/3\n",
    "    final_stats=pd.concat([final_stats,stats])\n",
    "    print(stats)  \n",
    "pickle.dump(final_stats,open(\"data/debug_embed_a\",\"wb\"))       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l,k=pairs(cases_training)\n",
    "matrix_x=np.zeros((len(l), 768))\n",
    "matrix_x=extract_cls(k,l,matrix_x,batch_size=32)\n",
    "pickle.dump(matrix_x,open(\"data/embeddings_final\",\"wb\"))  \n",
    "# matrix_x=pickle.load(open(\"data/embeddings_final\",\"rb\")) \n",
    "# matrix_y=pickle.load(open(\"data/embed_matrix_y\",\"rb\")) \n",
    "# binary=pickle.load(open(\"data/embed_binary\",\"rb\")) \n",
    "matrix_y,binary=matrix_y_def(matrix_x)\n",
    "\n",
    "print(len(matrix_x))\n",
    "print(len(matrix_y))\n",
    "if len(matrix_x)==len(matrix_y):\n",
    "    del matrix_x[\"pairs\"]\n",
    "    print(\"usunal pary\")\n",
    "\n",
    "\n",
    "coef=log_regr_train(matrix_x,5.9,\"a\",binary,matrix_y)\n",
    "\n",
    "\n",
    "stats,records=temp=calc_scores_embed(coef,cases_test,5.9)\n",
    "pickle.dump(stats,open(\"data/letor_a_embed_stats\",\"wb\")) \n",
    "pickle.dump(records,open(\"data/letor_a_embed_records\",\"wb\")) \n",
    "print(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f43bbfdbd25395a25ed823d9b4da9f0e87492724872f8769e910b96ff65c3c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
